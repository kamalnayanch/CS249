{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = False\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "28378\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read FastText and Made Dict\n",
      "25277\n",
      "3101\n"
     ]
    }
   ],
   "source": [
    "temp_name = 'embedding_matrix_fastext'\n",
    "create_w2vec = True\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/crawl-300d-2M.vec\"\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) !=301:\n",
    "            continue\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read FastText and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999995"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "25413\n",
      "49972\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text))\n",
    "print(len(train_dataset.stances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'train_full')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'test_full')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             8513700     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 300)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 601)          0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,574,304\n",
      "Trainable params: 60,604\n",
      "Non-trainable params: 8,513,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 396.00 556.00\" width=\"396pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 392,-552 392,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140086438501064 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140086438501064</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 185,-547.5 185,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-525.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140086438500056 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140086438500056</title>\n",
       "<polygon fill=\"none\" points=\"92,-438.5 92,-474.5 295,-474.5 295,-438.5 92,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-452.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140086438501064&#45;&gt;140086438500056 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140086438501064-&gt;140086438500056</title>\n",
       "<path d=\"M117.4663,-511.4551C130.3342,-502.1545 146.2038,-490.6844 160.1102,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.4164,-483.2849 168.4709,-474.5904 158.3159,-477.6116 162.4164,-483.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086438500280 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140086438500280</title>\n",
       "<polygon fill=\"none\" points=\"203,-511.5 203,-547.5 388,-547.5 388,-511.5 203,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-525.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140086438500280&#45;&gt;140086438500056 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140086438500280-&gt;140086438500056</title>\n",
       "<path d=\"M270.2865,-511.4551C257.2912,-502.1545 241.2645,-490.6844 227.2204,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.9459,-477.5641 218.7769,-474.5904 224.8719,-483.2565 228.9459,-477.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086468148696 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140086468148696</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-365.5 32.5,-401.5 184.5,-401.5 184.5,-365.5 32.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108.5\" y=\"-379.8\">lambda_1: Lambda</text>\n",
       "</g>\n",
       "<!-- 140086438500056&#45;&gt;140086468148696 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140086438500056-&gt;140086468148696</title>\n",
       "<path d=\"M172.4888,-438.4551C161.8636,-429.3299 148.8068,-418.1165 137.2645,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"139.4308,-405.4505 129.5641,-401.5904 134.8701,-410.7609 139.4308,-405.4505\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086437762160 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140086437762160</title>\n",
       "<polygon fill=\"none\" points=\"202.5,-365.5 202.5,-401.5 354.5,-401.5 354.5,-365.5 202.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-379.8\">lambda_2: Lambda</text>\n",
       "</g>\n",
       "<!-- 140086438500056&#45;&gt;140086437762160 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140086438500056-&gt;140086437762160</title>\n",
       "<path d=\"M214.5112,-438.4551C225.1364,-429.3299 238.1932,-418.1165 249.7355,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"252.1299,-410.7609 257.4359,-401.5904 247.5692,-405.4505 252.1299,-410.7609\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140089248288104 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140089248288104</title>\n",
       "<polygon fill=\"none\" points=\"153,-292.5 153,-328.5 244,-328.5 244,-292.5 153,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-306.8\">dot_1: Dot</text>\n",
       "</g>\n",
       "<!-- 140086468148696&#45;&gt;140089248288104 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140086468148696-&gt;140089248288104</title>\n",
       "<path d=\"M130.7472,-365.4551C142.1055,-356.2422 156.0883,-344.9006 168.3956,-334.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"170.6352,-337.6081 176.1968,-328.5904 166.2256,-332.1716 170.6352,-337.6081\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086438808712 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140086438808712</title>\n",
       "<polygon fill=\"none\" points=\"88,-219.5 88,-255.5 309,-255.5 309,-219.5 88,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-233.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140086468148696&#45;&gt;140086438808712 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140086468148696-&gt;140086438808712</title>\n",
       "<path d=\"M113.0076,-365.2421C118.3117,-345.9693 128.3617,-315.2248 143.5,-292 150.439,-281.3544 159.7529,-271.1395 168.7416,-262.4888\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.2023,-264.9803 176.1563,-255.615 166.4433,-259.8468 171.2023,-264.9803\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086437762160&#45;&gt;140089248288104 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140086437762160-&gt;140089248288104</title>\n",
       "<path d=\"M258.7247,-365.4551C248.8207,-356.4177 236.6719,-345.3319 225.8862,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.0711,-332.7455 218.3251,-328.5904 223.3527,-337.9163 228.0711,-332.7455\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086437762160&#45;&gt;140086438808712 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140086437762160-&gt;140086438808712</title>\n",
       "<path d=\"M276.076,-365.2487C272.9079,-345.9813 266.0807,-315.2413 252.5,-292 246.2629,-281.3263 237.4505,-271.2285 228.7499,-262.692\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"230.8903,-259.9011 221.2003,-255.6169 226.1037,-265.0088 230.8903,-259.9011\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140089248288104&#45;&gt;140086438808712 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140089248288104-&gt;140086438808712</title>\n",
       "<path d=\"M198.5,-292.4551C198.5,-284.3828 198.5,-274.6764 198.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-265.5903 198.5,-255.5904 195.0001,-265.5904 202.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086439493248 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140086439493248</title>\n",
       "<polygon fill=\"none\" points=\"134.5,-146.5 134.5,-182.5 262.5,-182.5 262.5,-146.5 134.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140086438808712&#45;&gt;140086439493248 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140086438808712-&gt;140086439493248</title>\n",
       "<path d=\"M198.5,-219.4551C198.5,-211.3828 198.5,-201.6764 198.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-192.5903 198.5,-182.5904 195.0001,-192.5904 202.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086438498712 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140086438498712</title>\n",
       "<polygon fill=\"none\" points=\"120,-73.5 120,-109.5 277,-109.5 277,-73.5 120,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140086439493248&#45;&gt;140086438498712 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140086439493248-&gt;140086438498712</title>\n",
       "<path d=\"M198.5,-146.4551C198.5,-138.3828 198.5,-128.6764 198.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-119.5903 198.5,-109.5904 195.0001,-119.5904 202.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140086437016520 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140086437016520</title>\n",
       "<polygon fill=\"none\" points=\"134.5,-.5 134.5,-36.5 262.5,-36.5 262.5,-.5 134.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140086438498712&#45;&gt;140086437016520 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140086438498712-&gt;140086437016520</title>\n",
       "<path d=\"M198.5,-73.4551C198.5,-65.3828 198.5,-55.6764 198.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-46.5903 198.5,-36.5904 195.0001,-46.5904 202.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "head_embed = Lambda(adder,output_shape=adder_output)(head_embed)\n",
    "body_embed = Lambda(adder,output_shape=adder_output)(body_embed)\n",
    "dot = dot([head_embed,body_embed],axes = 1, normalize=True)\n",
    "conc = concatenate([head_embed,body_embed,dot])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.33)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[head_input, body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.7094 - acc: 0.7468 - val_loss: 0.6180 - val_acc: 0.7582\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 290us/step - loss: 0.4543 - acc: 0.8378 - val_loss: 0.4472 - val_acc: 0.8479\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 289us/step - loss: 0.3532 - acc: 0.8782 - val_loss: 0.4204 - val_acc: 0.8546\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 287us/step - loss: 0.3074 - acc: 0.8939 - val_loss: 0.4005 - val_acc: 0.8624\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 15s 297us/step - loss: 0.2832 - acc: 0.9014 - val_loss: 0.4008 - val_acc: 0.8605\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 15s 293us/step - loss: 0.2621 - acc: 0.9078 - val_loss: 0.4095 - val_acc: 0.8590\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.2480 - acc: 0.9126 - val_loss: 0.4118 - val_acc: 0.8577\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 15s 292us/step - loss: 0.2368 - acc: 0.9151 - val_loss: 0.4192 - val_acc: 0.8546\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 15s 295us/step - loss: 0.2268 - acc: 0.9197 - val_loss: 0.4211 - val_acc: 0.8565\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.2154 - acc: 0.9221 - val_loss: 0.4364 - val_acc: 0.8548\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1042    |     0     |    618    |    243    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    303    |     7     |    201    |    186    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    681    |     3     |   3231    |    549    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    169    |     2     |    735    |   17443   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9092.25 out of 11651.25\t(78.03669134213067%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.2074 - acc: 0.9252 - val_loss: 0.4502 - val_acc: 0.8518\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 15s 297us/step - loss: 0.1996 - acc: 0.9290 - val_loss: 0.4470 - val_acc: 0.8565\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 288us/step - loss: 0.1937 - acc: 0.9290 - val_loss: 0.4605 - val_acc: 0.8475\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.1842 - acc: 0.9346 - val_loss: 0.4628 - val_acc: 0.8566\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.1803 - acc: 0.9352 - val_loss: 0.4726 - val_acc: 0.8527\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 15s 295us/step - loss: 0.1731 - acc: 0.9379 - val_loss: 0.4635 - val_acc: 0.8545\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.1702 - acc: 0.9384 - val_loss: 0.4805 - val_acc: 0.8504\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.1631 - acc: 0.9409 - val_loss: 0.4908 - val_acc: 0.8483\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.1592 - acc: 0.9422 - val_loss: 0.5004 - val_acc: 0.8422\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.1548 - acc: 0.9435 - val_loss: 0.4896 - val_acc: 0.8476\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1100    |    24     |    540    |    239    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    281    |    19     |    208    |    189    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    766    |    19     |   3186    |    493    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    218    |    11     |    884    |   17236   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9073.5 out of 11651.25\t(77.87576440296105%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.1509 - acc: 0.9443 - val_loss: 0.5072 - val_acc: 0.8493\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 290us/step - loss: 0.1477 - acc: 0.9460 - val_loss: 0.5099 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 15s 293us/step - loss: 0.1432 - acc: 0.9487 - val_loss: 0.5348 - val_acc: 0.8386\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 290us/step - loss: 0.1391 - acc: 0.9500 - val_loss: 0.5493 - val_acc: 0.8334\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 290us/step - loss: 0.1347 - acc: 0.9518 - val_loss: 0.5366 - val_acc: 0.8453\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 15s 293us/step - loss: 0.1331 - acc: 0.9515 - val_loss: 0.5421 - val_acc: 0.8471\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.1291 - acc: 0.9538 - val_loss: 0.5312 - val_acc: 0.8500\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 15s 292us/step - loss: 0.1260 - acc: 0.9550 - val_loss: 0.5495 - val_acc: 0.8474\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 15s 292us/step - loss: 0.1220 - acc: 0.9554 - val_loss: 0.5596 - val_acc: 0.8436\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 287us/step - loss: 0.1215 - acc: 0.9558 - val_loss: 0.5567 - val_acc: 0.8451\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1063    |    25     |    534    |    281    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    281    |    18     |    188    |    210    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    739    |    21     |   3099    |    605    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    215    |    19     |    819    |   17296   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8951.0 out of 11651.25\t(76.8243750670529%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 14s 287us/step - loss: 0.1169 - acc: 0.9588 - val_loss: 0.5750 - val_acc: 0.8435\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 282us/step - loss: 0.1157 - acc: 0.9578 - val_loss: 0.5655 - val_acc: 0.8426\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 289us/step - loss: 0.1136 - acc: 0.9587 - val_loss: 0.5673 - val_acc: 0.8440\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 15s 292us/step - loss: 0.1117 - acc: 0.9596 - val_loss: 0.5864 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 284us/step - loss: 0.1089 - acc: 0.9613 - val_loss: 0.5690 - val_acc: 0.8457\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.1067 - acc: 0.9609 - val_loss: 0.5894 - val_acc: 0.8376\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 16s 312us/step - loss: 0.1054 - acc: 0.9619 - val_loss: 0.5888 - val_acc: 0.8468\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 16s 313us/step - loss: 0.1048 - acc: 0.9624 - val_loss: 0.6021 - val_acc: 0.8395\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.0994 - acc: 0.9647 - val_loss: 0.6241 - val_acc: 0.8421\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.0988 - acc: 0.9644 - val_loss: 0.6252 - val_acc: 0.8426\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    998    |    21     |    536    |    348    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    235    |    17     |    191    |    254    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    724    |    16     |   3035    |    689    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    247    |     6     |    734    |   17362   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8821.25 out of 11651.25\t(75.71076064799914%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0971 - acc: 0.9653 - val_loss: 0.6483 - val_acc: 0.8443\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 15s 305us/step - loss: 0.0963 - acc: 0.9654 - val_loss: 0.6219 - val_acc: 0.8455\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.0935 - acc: 0.9672 - val_loss: 0.6466 - val_acc: 0.8413\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0901 - acc: 0.9677 - val_loss: 0.6566 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0899 - acc: 0.9668 - val_loss: 0.6603 - val_acc: 0.8444\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.0905 - acc: 0.9673 - val_loss: 0.6634 - val_acc: 0.8404\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.0871 - acc: 0.9692 - val_loss: 0.6849 - val_acc: 0.8333\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 16s 313us/step - loss: 0.0856 - acc: 0.9698 - val_loss: 0.6741 - val_acc: 0.8395\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0835 - acc: 0.9710 - val_loss: 0.6694 - val_acc: 0.8387\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0840 - acc: 0.9705 - val_loss: 0.7041 - val_acc: 0.8360\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1115    |    20     |    464    |    304    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    300    |    14     |    164    |    219    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    939    |    21     |   2850    |    654    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    339    |     9     |    735    |   17266   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8772.5 out of 11651.25\t(75.29235060615814%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=10, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.0823 - acc: 0.9707 - val_loss: 0.6984 - val_acc: 0.8438\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 296us/step - loss: 0.0816 - acc: 0.9711 - val_loss: 0.6946 - val_acc: 0.8382\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 14s 288us/step - loss: 0.0789 - acc: 0.9718 - val_loss: 0.6861 - val_acc: 0.8409\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 293us/step - loss: 0.0783 - acc: 0.9728 - val_loss: 0.6960 - val_acc: 0.8433\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 294us/step - loss: 0.0785 - acc: 0.9722 - val_loss: 0.7056 - val_acc: 0.8426\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1070    |    29     |    488    |    316    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    257    |    20     |    189    |    231    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    805    |    33     |   3002    |    624    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    301    |    14     |    712    |   17322   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8872.75 out of 11651.25\t(76.15277330758502%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 16s 328us/step - loss: 0.0751 - acc: 0.9737 - val_loss: 0.7150 - val_acc: 0.8347\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 307us/step - loss: 0.0761 - acc: 0.9736 - val_loss: 0.7163 - val_acc: 0.8445\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0711 - acc: 0.9752 - val_loss: 0.7221 - val_acc: 0.8443\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.0737 - acc: 0.9727 - val_loss: 0.7364 - val_acc: 0.8381\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 16s 318us/step - loss: 0.0707 - acc: 0.9753 - val_loss: 0.7272 - val_acc: 0.8407\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1031    |    33     |    540    |    299    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    255    |    26     |    200    |    216    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    735    |    39     |   3127    |    563    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    302    |    16     |    850    |   17181   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8929.75 out of 11651.25\t(76.64199120266066%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.0723 - acc: 0.9743 - val_loss: 0.7423 - val_acc: 0.8411\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 306us/step - loss: 0.0698 - acc: 0.9750 - val_loss: 0.7358 - val_acc: 0.8360\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0687 - acc: 0.9763 - val_loss: 0.7781 - val_acc: 0.8434\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.0677 - acc: 0.9759 - val_loss: 0.7602 - val_acc: 0.8416\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 17s 342us/step - loss: 0.0673 - acc: 0.9763 - val_loss: 0.8082 - val_acc: 0.8393\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    994    |    24     |    489    |    396    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    229    |    24     |    166    |    278    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    716    |    43     |   2885    |    820    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    245    |    14     |    665    |   17425   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8676.0 out of 11651.25\t(74.46411329256517%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.0662 - acc: 0.9766 - val_loss: 0.7829 - val_acc: 0.8435\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 310us/step - loss: 0.0638 - acc: 0.9772 - val_loss: 0.8231 - val_acc: 0.8385\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 16s 311us/step - loss: 0.0645 - acc: 0.9776 - val_loss: 0.7707 - val_acc: 0.8428\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 301us/step - loss: 0.0647 - acc: 0.9770 - val_loss: 0.7950 - val_acc: 0.8353\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.0625 - acc: 0.9778 - val_loss: 0.7962 - val_acc: 0.8402\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1005    |    25     |    527    |    346    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    232    |    19     |    190    |    256    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    744    |    42     |   2943    |    735    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    314    |    13     |    636    |   17386   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8753.5 out of 11651.25\t(75.12927797446626%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 294us/step - loss: 0.0630 - acc: 0.9775 - val_loss: 0.8331 - val_acc: 0.8385\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 294us/step - loss: 0.0611 - acc: 0.9792 - val_loss: 0.8062 - val_acc: 0.8387\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 14s 285us/step - loss: 0.0606 - acc: 0.9789 - val_loss: 0.8077 - val_acc: 0.8381\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.0614 - acc: 0.9779 - val_loss: 0.7952 - val_acc: 0.8413\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 308us/step - loss: 0.0604 - acc: 0.9788 - val_loss: 0.8186 - val_acc: 0.8352\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    966    |    27     |    504    |    406    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    210    |    28     |    171    |    288    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    687    |    50     |   2882    |    845    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    284    |    15     |    700    |   17350   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8625.75 out of 11651.25\t(74.0328290955906%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=5, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
