{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = False\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "28378\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = False\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "25413\n",
      "49972\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text))\n",
    "print(len(train_dataset.stances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'train_full')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'test_full')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        multiple             8513700     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 300)          0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 300)          0           embedding_11[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1)            0           lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 601)          0           lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          60200       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 100)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 4)            404         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,574,304\n",
      "Trainable params: 60,604\n",
      "Non-trainable params: 8,513,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 396.00 556.00\" width=\"396pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 392,-552 392,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139623634885488 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139623634885488</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 185,-547.5 185,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-525.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139623634885096 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139623634885096</title>\n",
       "<polygon fill=\"none\" points=\"87.5,-438.5 87.5,-474.5 299.5,-474.5 299.5,-438.5 87.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-452.8\">embedding_11: Embedding</text>\n",
       "</g>\n",
       "<!-- 139623634885488&#45;&gt;139623634885096 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139623634885488-&gt;139623634885096</title>\n",
       "<path d=\"M117.4663,-511.4551C130.3342,-502.1545 146.2038,-490.6844 160.1102,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.4164,-483.2849 168.4709,-474.5904 158.3159,-477.6116 162.4164,-483.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634884592 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139623634884592</title>\n",
       "<polygon fill=\"none\" points=\"203,-511.5 203,-547.5 388,-547.5 388,-511.5 203,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-525.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139623634884592&#45;&gt;139623634885096 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139623634884592-&gt;139623634885096</title>\n",
       "<path d=\"M270.2865,-511.4551C257.2912,-502.1545 241.2645,-490.6844 227.2204,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.9459,-477.5641 218.7769,-474.5904 224.8719,-483.2565 228.9459,-477.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634883920 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139623634883920</title>\n",
       "<polygon fill=\"none\" points=\"24,-365.5 24,-401.5 185,-401.5 185,-365.5 24,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-379.8\">lambda_21: Lambda</text>\n",
       "</g>\n",
       "<!-- 139623634885096&#45;&gt;139623634883920 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139623634885096-&gt;139623634883920</title>\n",
       "<path d=\"M171.5,-438.4551C160.3749,-429.3299 146.7036,-418.1165 134.6181,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.5069,-405.2261 126.5554,-401.5904 132.0676,-410.6384 136.5069,-405.2261\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634884648 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139623634884648</title>\n",
       "<polygon fill=\"none\" points=\"203,-365.5 203,-401.5 364,-401.5 364,-365.5 203,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-379.8\">lambda_22: Lambda</text>\n",
       "</g>\n",
       "<!-- 139623634885096&#45;&gt;139623634884648 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139623634885096-&gt;139623634884648</title>\n",
       "<path d=\"M215.7472,-438.4551C227.1055,-429.2422 241.0883,-417.9006 253.3956,-407.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"255.6352,-410.6081 261.1968,-401.5904 251.2256,-405.1716 255.6352,-410.6081\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634885152 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139623634885152</title>\n",
       "<polygon fill=\"none\" points=\"156,-292.5 156,-328.5 247,-328.5 247,-292.5 156,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-306.8\">dot_6: Dot</text>\n",
       "</g>\n",
       "<!-- 139623634883920&#45;&gt;139623634885152 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139623634883920-&gt;139623634885152</title>\n",
       "<path d=\"M128.4775,-365.4551C140.8359,-356.1545 156.0769,-344.6844 169.4326,-334.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.5766,-337.4001 177.4621,-328.5904 167.3674,-331.807 171.5766,-337.4001\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634884816 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139623634884816</title>\n",
       "<polygon fill=\"none\" points=\"91,-219.5 91,-255.5 312,-255.5 312,-219.5 91,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-233.8\">concatenate_3: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139623634883920&#45;&gt;139623634884816 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139623634883920-&gt;139623634884816</title>\n",
       "<path d=\"M110.6381,-365.2917C117.6117,-346.0603 130.1752,-315.35 146.5,-292 153.8413,-281.4994 163.3401,-271.2632 172.3722,-262.55\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"174.8748,-265.0017 179.7917,-255.6168 170.0956,-259.8871 174.8748,-265.0017\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634884648&#45;&gt;139623634885152 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139623634884648-&gt;139623634885152</title>\n",
       "<path d=\"M263.2303,-365.4551C253.0788,-356.4177 240.6262,-345.3319 229.5709,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.617,-332.6255 221.8207,-328.5904 226.9625,-337.8539 231.617,-332.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634884648&#45;&gt;139623634884816 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139623634884648-&gt;139623634884816</title>\n",
       "<path d=\"M280.6007,-365.2486C276.9477,-345.9812 269.3959,-315.2411 255.5,-292 249.156,-281.3895 240.3141,-271.3091 231.6195,-262.7691\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"233.767,-259.9842 224.0832,-255.6861 228.9731,-265.085 233.767,-259.9842\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139623634885152&#45;&gt;139623634884816 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139623634885152-&gt;139623634884816</title>\n",
       "<path d=\"M201.5,-292.4551C201.5,-284.3828 201.5,-274.6764 201.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"205.0001,-265.5903 201.5,-255.5904 198.0001,-265.5904 205.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139622413185544 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139622413185544</title>\n",
       "<polygon fill=\"none\" points=\"133,-146.5 133,-182.5 270,-182.5 270,-146.5 133,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-160.8\">dense_13: Dense</text>\n",
       "</g>\n",
       "<!-- 139623634884816&#45;&gt;139622413185544 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>139623634884816-&gt;139622413185544</title>\n",
       "<path d=\"M201.5,-219.4551C201.5,-211.3828 201.5,-201.6764 201.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"205.0001,-192.5903 201.5,-182.5904 198.0001,-192.5904 205.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139622413185096 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139622413185096</title>\n",
       "<polygon fill=\"none\" points=\"123,-73.5 123,-109.5 280,-109.5 280,-73.5 123,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-87.8\">dropout_7: Dropout</text>\n",
       "</g>\n",
       "<!-- 139622413185544&#45;&gt;139622413185096 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>139622413185544-&gt;139622413185096</title>\n",
       "<path d=\"M201.5,-146.4551C201.5,-138.3828 201.5,-128.6764 201.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"205.0001,-119.5903 201.5,-109.5904 198.0001,-119.5904 205.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139622411164752 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>139622411164752</title>\n",
       "<polygon fill=\"none\" points=\"133,-.5 133,-36.5 270,-36.5 270,-.5 133,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-14.8\">dense_14: Dense</text>\n",
       "</g>\n",
       "<!-- 139622413185096&#45;&gt;139622411164752 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>139622413185096-&gt;139622411164752</title>\n",
       "<path d=\"M201.5,-73.4551C201.5,-65.3828 201.5,-55.6764 201.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"205.0001,-46.5903 201.5,-36.5904 198.0001,-46.5904 205.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "head_embed = Lambda(adder,output_shape=adder_output)(head_embed)\n",
    "body_embed = Lambda(adder,output_shape=adder_output)(body_embed)\n",
    "dot = dot([head_embed,body_embed],axes = 1, normalize=True)\n",
    "conc = concatenate([head_embed,body_embed,dot])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.33)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[head_input, body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 10s 194us/step - loss: 0.3835 - acc: 0.8557\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 10s 192us/step - loss: 0.3686 - acc: 0.8629\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 11s 212us/step - loss: 0.3529 - acc: 0.8678\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 34s 686us/step - loss: 0.3377 - acc: 0.8749\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 11s 223us/step - loss: 0.3259 - acc: 0.8788\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 11s 226us/step - loss: 0.3107 - acc: 0.8849\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 11s 219us/step - loss: 0.3001 - acc: 0.8890\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 10s 209us/step - loss: 0.2911 - acc: 0.8943\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 10s 204us/step - loss: 0.2815 - acc: 0.8968\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 10s 202us/step - loss: 0.2753 - acc: 0.8984\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    471    |    25     |    251    |   1156    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    102    |    14     |    62     |    519    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    474    |    21     |   1647    |   2322    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    247    |     4     |    600    |   17498   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6740.25 out of 11651.25\t(57.85001609269392%)\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 10s 196us/step - loss: 0.2661 - acc: 0.9025\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.2585 - acc: 0.9068\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 10s 196us/step - loss: 0.2526 - acc: 0.9076\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.2467 - acc: 0.9102\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 10s 194us/step - loss: 0.2416 - acc: 0.9116\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 10s 191us/step - loss: 0.2352 - acc: 0.9144\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 10s 193us/step - loss: 0.2301 - acc: 0.9158\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 10s 206us/step - loss: 0.2231 - acc: 0.9187\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 11s 216us/step - loss: 0.2202 - acc: 0.9191\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 11s 216us/step - loss: 0.2156 - acc: 0.9213\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    611    |    58     |    293    |    941    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    151    |    39     |    90     |    417    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    756    |    66     |   1794    |   1848    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    578    |    46     |    847    |   16878   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7017.0 out of 11651.25\t(60.22529771483747%)\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.2108 - acc: 0.9250\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 11s 211us/step - loss: 0.2061 - acc: 0.9254\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 10s 193us/step - loss: 0.2034 - acc: 0.9258\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1974 - acc: 0.9283\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 10s 201us/step - loss: 0.1944 - acc: 0.9294\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 10s 198us/step - loss: 0.1892 - acc: 0.9313\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1885 - acc: 0.9317\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 10s 200us/step - loss: 0.1843 - acc: 0.9335\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 11s 219us/step - loss: 0.1808 - acc: 0.9359\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1788 - acc: 0.9347\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    678    |    43     |    354    |    828    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    153    |    34     |    112    |    398    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    874    |    68     |   1901    |   1621    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    816    |    69     |   1167    |   16297   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7088.25 out of 11651.25\t(60.83682008368201%)\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1771 - acc: 0.9352\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 10s 201us/step - loss: 0.1729 - acc: 0.9370\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 10s 192us/step - loss: 0.1701 - acc: 0.9379\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 10s 192us/step - loss: 0.1669 - acc: 0.9392\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 10s 194us/step - loss: 0.1646 - acc: 0.9396\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 10s 193us/step - loss: 0.1625 - acc: 0.9416\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 10s 196us/step - loss: 0.1596 - acc: 0.9424\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 10s 192us/step - loss: 0.1591 - acc: 0.9429\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1531 - acc: 0.9451\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1525 - acc: 0.9440\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    622    |    26     |    356    |    899    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    152    |    23     |    104    |    418    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    801    |    60     |   1777    |   1826    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    621    |    59     |   1046    |   16623   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6952.5 out of 11651.25\t(59.67170904409398%)\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1508 - acc: 0.9464\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1480 - acc: 0.9459\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1469 - acc: 0.9466\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 10s 197us/step - loss: 0.1443 - acc: 0.9484\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 10s 194us/step - loss: 0.1395 - acc: 0.9491\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1410 - acc: 0.9494\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 10s 195us/step - loss: 0.1407 - acc: 0.9488\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 10s 203us/step - loss: 0.1379 - acc: 0.9501\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 10s 196us/step - loss: 0.1363 - acc: 0.9506\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 10s 196us/step - loss: 0.1328 - acc: 0.9524\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    587    |    23     |    270    |   1023    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    137    |    27     |    79     |    454    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    794    |    71     |   1673    |   1926    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    590    |    64     |    902    |   16793   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6828.75 out of 11651.25\t(58.60959124557451%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=10, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.6864 - acc: 0.7629 - val_loss: 0.5043 - val_acc: 0.8275\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.4072 - acc: 0.8615 - val_loss: 0.4007 - val_acc: 0.8599\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.3469 - acc: 0.8810 - val_loss: 0.3878 - val_acc: 0.8636\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.3229 - acc: 0.8876 - val_loss: 0.3889 - val_acc: 0.8627\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.3093 - acc: 0.8923 - val_loss: 0.3839 - val_acc: 0.8622\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1082    |     0     |    594    |    227    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    275    |     0     |    218    |    204    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    798    |     0     |   2977    |    689    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    170    |     0     |    326    |   17853   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8993.5 out of 11651.25\t(77.18914279583736%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.2945 - acc: 0.8950 - val_loss: 0.3941 - val_acc: 0.8587\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.2834 - acc: 0.8988 - val_loss: 0.3982 - val_acc: 0.8578\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.2730 - acc: 0.9014 - val_loss: 0.3921 - val_acc: 0.8563\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.2628 - acc: 0.9047 - val_loss: 0.4028 - val_acc: 0.8513\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.2552 - acc: 0.9079 - val_loss: 0.3994 - val_acc: 0.8565\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    988    |     3     |    645    |    267    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    248    |     7     |    245    |    197    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    887    |     1     |   2873    |    703    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    106    |     0     |    344    |   17899   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8850.0 out of 11651.25\t(75.95751528805923%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.2458 - acc: 0.9105 - val_loss: 0.4100 - val_acc: 0.8517\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.2387 - acc: 0.9126 - val_loss: 0.4087 - val_acc: 0.8522\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.2331 - acc: 0.9141 - val_loss: 0.4110 - val_acc: 0.8537\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 14s 280us/step - loss: 0.2284 - acc: 0.9171 - val_loss: 0.4128 - val_acc: 0.8533\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.2216 - acc: 0.9189 - val_loss: 0.4173 - val_acc: 0.8516\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    948    |     4     |    751    |    200    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    243    |    24     |    287    |    143    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    936    |    10     |   3013    |    505    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    128    |     1     |    563    |   17657   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8957.0 out of 11651.25\t(76.87587168758716%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      " 5184/49972 [==>...........................] - ETA: 8s - loss: 0.2134 - acc: 0.9207"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b5836e036ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_body\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mevaluate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=5, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
