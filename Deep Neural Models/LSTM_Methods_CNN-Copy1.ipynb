{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from fncbaseline import feature_engineering\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = False\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "28378\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = False\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "30\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text[0]))\n",
    "print(len(train_body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'trainFull')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'testFull')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_labels(train_dataset)\n",
    "y_test = create_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t1\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_features = np.load('train_features_sentiment.npy')\n",
    "test_sentiment_features = np.load('test_features_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         multiple             8513700     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv11 (Conv1D)                 (None, 26, 64)       96064       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv12 (Conv1D)                 (None, 496, 64)      96064       embedding_6[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling1D)            multiple             0           conv11[0][0]                     \n",
      "                                                                 conv12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv21 (Conv1D)                 (None, 10, 32)       6176        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv22 (Conv1D)                 (None, 245, 32)      6176        pool3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling1D)            multiple             0           conv21[0][0]                     \n",
      "                                                                 conv22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 128)          0           pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 3904)         0           pool4[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4032)         0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          403300      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            404         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,121,884\n",
      "Trainable params: 608,184\n",
      "Non-trainable params: 8,513,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 396.00 775.00\" width=\"396pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 392,-771 392,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140551322694712 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140551322694712</title>\n",
       "<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 185,-766.5 185,-730.5 0,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-744.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140551319260072 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140551319260072</title>\n",
       "<polygon fill=\"none\" points=\"92,-657.5 92,-693.5 295,-693.5 295,-657.5 92,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-671.8\">embedding_6: Embedding</text>\n",
       "</g>\n",
       "<!-- 140551322694712&#45;&gt;140551319260072 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140551322694712-&gt;140551319260072</title>\n",
       "<path d=\"M117.4663,-730.4551C130.3342,-721.1545 146.2038,-709.6844 160.1102,-699.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.4164,-702.2849 168.4709,-693.5904 158.3159,-696.6116 162.4164,-702.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322694824 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140551322694824</title>\n",
       "<polygon fill=\"none\" points=\"203,-730.5 203,-766.5 388,-766.5 388,-730.5 203,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-744.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140551322694824&#45;&gt;140551319260072 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140551322694824-&gt;140551319260072</title>\n",
       "<path d=\"M270.2865,-730.4551C257.2912,-721.1545 241.2645,-709.6844 227.2204,-699.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.9459,-696.5641 218.7769,-693.5904 224.8719,-702.2565 228.9459,-696.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322694768 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140551322694768</title>\n",
       "<polygon fill=\"none\" points=\"50.5,-584.5 50.5,-620.5 184.5,-620.5 184.5,-584.5 50.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117.5\" y=\"-598.8\">conv11: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140551319260072&#45;&gt;140551322694768 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140551319260072-&gt;140551322694768</title>\n",
       "<path d=\"M174.7135,-657.4551C165.3961,-648.5054 153.9873,-637.547 143.8157,-627.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"145.9704,-624.9935 136.3338,-620.5904 141.1212,-630.0419 145.9704,-624.9935\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322694208 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140551322694208</title>\n",
       "<polygon fill=\"none\" points=\"202.5,-584.5 202.5,-620.5 336.5,-620.5 336.5,-584.5 202.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-598.8\">conv12: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140551319260072&#45;&gt;140551322694208 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140551319260072-&gt;140551322694208</title>\n",
       "<path d=\"M212.2865,-657.4551C221.6039,-648.5054 233.0127,-637.547 243.1843,-627.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.8788,-630.0419 250.6662,-620.5904 241.0296,-624.9935 245.8788,-630.0419\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322729664 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140551322729664</title>\n",
       "<polygon fill=\"none\" points=\"109.5,-511.5 109.5,-547.5 277.5,-547.5 277.5,-511.5 109.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-525.8\">pool3: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140551322694768&#45;&gt;140551322729664 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140551322694768-&gt;140551322729664</title>\n",
       "<path d=\"M136.2865,-584.4551C145.6039,-575.5054 157.0127,-564.547 167.1843,-554.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"169.8788,-557.0419 174.6662,-547.5904 165.0296,-551.9935 169.8788,-557.0419\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322694208&#45;&gt;140551322729664 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140551322694208-&gt;140551322729664</title>\n",
       "<path d=\"M250.7135,-584.4551C241.3961,-575.5054 229.9873,-564.547 219.8157,-554.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"221.9704,-551.9935 212.3338,-547.5904 217.1212,-557.0419 221.9704,-551.9935\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551323180952 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140551323180952</title>\n",
       "<polygon fill=\"none\" points=\"50.5,-438.5 50.5,-474.5 184.5,-474.5 184.5,-438.5 50.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117.5\" y=\"-452.8\">conv21: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140551322729664&#45;&gt;140551323180952 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140551322729664-&gt;140551323180952</title>\n",
       "<path d=\"M174.7135,-511.4551C165.3961,-502.5054 153.9873,-491.547 143.8157,-481.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"145.9704,-478.9935 136.3338,-474.5904 141.1212,-484.0419 145.9704,-478.9935\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551323179888 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140551323179888</title>\n",
       "<polygon fill=\"none\" points=\"202.5,-438.5 202.5,-474.5 336.5,-474.5 336.5,-438.5 202.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-452.8\">conv22: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140551322729664&#45;&gt;140551323179888 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140551322729664-&gt;140551323179888</title>\n",
       "<path d=\"M212.2865,-511.4551C221.6039,-502.5054 233.0127,-491.547 243.1843,-481.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.8788,-484.0419 250.6662,-474.5904 241.0296,-478.9935 245.8788,-484.0419\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322730168 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140551322730168</title>\n",
       "<polygon fill=\"none\" points=\"109.5,-365.5 109.5,-401.5 277.5,-401.5 277.5,-365.5 109.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-379.8\">pool4: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140551323180952&#45;&gt;140551322730168 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140551323180952-&gt;140551322730168</title>\n",
       "<path d=\"M136.2865,-438.4551C145.6039,-429.5054 157.0127,-418.547 167.1843,-408.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"169.8788,-411.0419 174.6662,-401.5904 165.0296,-405.9935 169.8788,-411.0419\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551323179888&#45;&gt;140551322730168 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140551323179888-&gt;140551322730168</title>\n",
       "<path d=\"M250.7135,-438.4551C241.3961,-429.5054 229.9873,-418.547 219.8157,-408.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"221.9704,-405.9935 212.3338,-401.5904 217.1212,-411.0419 221.9704,-405.9935\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551318037784 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140551318037784</title>\n",
       "<polygon fill=\"none\" points=\"41,-292.5 41,-328.5 182,-328.5 182,-292.5 41,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-306.8\">flatten_9: Flatten</text>\n",
       "</g>\n",
       "<!-- 140551322730168&#45;&gt;140551318037784 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140551322730168-&gt;140551318037784</title>\n",
       "<path d=\"M173.2303,-365.4551C163.0788,-356.4177 150.6262,-345.3319 139.5709,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"141.617,-332.6255 131.8207,-328.5904 136.9625,-337.8539 141.617,-332.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551293639592 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140551293639592</title>\n",
       "<polygon fill=\"none\" points=\"200.5,-292.5 200.5,-328.5 350.5,-328.5 350.5,-292.5 200.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-306.8\">flatten_10: Flatten</text>\n",
       "</g>\n",
       "<!-- 140551322730168&#45;&gt;140551293639592 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140551322730168-&gt;140551293639592</title>\n",
       "<path d=\"M213.7697,-365.4551C223.9212,-356.4177 236.3738,-345.3319 247.4291,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"250.0375,-337.8539 255.1793,-328.5904 245.383,-332.6255 250.0375,-337.8539\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322730280 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140551322730280</title>\n",
       "<polygon fill=\"none\" points=\"83,-219.5 83,-255.5 304,-255.5 304,-219.5 83,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-233.8\">concatenate_4: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140551318037784&#45;&gt;140551322730280 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140551318037784-&gt;140551322730280</title>\n",
       "<path d=\"M131.7697,-292.4551C141.9212,-283.4177 154.3738,-272.3319 165.4291,-262.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"168.0375,-264.8539 173.1793,-255.5904 163.383,-259.6255 168.0375,-264.8539\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551293639592&#45;&gt;140551322730280 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140551293639592-&gt;140551322730280</title>\n",
       "<path d=\"M255.2303,-292.4551C245.0788,-283.4177 232.6262,-272.3319 221.5709,-262.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"223.617,-259.6255 213.8207,-255.5904 218.9625,-264.8539 223.617,-259.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551322730392 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140551322730392</title>\n",
       "<polygon fill=\"none\" points=\"129.5,-146.5 129.5,-182.5 257.5,-182.5 257.5,-146.5 129.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-160.8\">dense_7: Dense</text>\n",
       "</g>\n",
       "<!-- 140551322730280&#45;&gt;140551322730392 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140551322730280-&gt;140551322730392</title>\n",
       "<path d=\"M193.5,-219.4551C193.5,-211.3828 193.5,-201.6764 193.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"197.0001,-192.5903 193.5,-182.5904 190.0001,-192.5904 197.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551320869912 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140551320869912</title>\n",
       "<polygon fill=\"none\" points=\"115,-73.5 115,-109.5 272,-109.5 272,-73.5 115,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-87.8\">dropout_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 140551322730392&#45;&gt;140551320869912 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140551322730392-&gt;140551320869912</title>\n",
       "<path d=\"M193.5,-146.4551C193.5,-138.3828 193.5,-128.6764 193.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"197.0001,-119.5903 193.5,-109.5904 190.0001,-119.5904 197.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140551321046432 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140551321046432</title>\n",
       "<polygon fill=\"none\" points=\"129.5,-.5 129.5,-36.5 257.5,-36.5 257.5,-.5 129.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-14.8\">dense_8: Dense</text>\n",
       "</g>\n",
       "<!-- 140551320869912&#45;&gt;140551321046432 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140551320869912-&gt;140551321046432</title>\n",
       "<path d=\"M193.5,-73.4551C193.5,-65.3828 193.5,-55.6764 193.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"197.0001,-46.5903 193.5,-36.5904 190.0001,-46.5904 197.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "\n",
    "conv_11 = Conv1D(64, 5, activation='relu', name='conv11')\n",
    "conv_12 = Conv1D(64, 5, activation='relu', name='conv12')\n",
    "conv_21 = Conv1D(32, 3, activation='relu', name='conv21')\n",
    "conv_22 = Conv1D(32, 3, activation='relu', name='conv22')\n",
    "#pooling layers\n",
    "pool_3 = MaxPooling1D(pool_size=3, strides=2, name='pool3')\n",
    "pool_4 = MaxPooling1D(pool_size=3, strides=2, name='pool4')\n",
    "\n",
    "\n",
    "head_CNN = conv_11(head_embed)\n",
    "head_CNN = pool_3(head_CNN)\n",
    "head_CNN = conv_21(head_CNN)\n",
    "head_CNN = pool_4(head_CNN)\n",
    "head_CNN = Flatten()(head_CNN)\n",
    "\n",
    "body_CNN = conv_12(body_embed)\n",
    "body_CNN = pool_3(body_CNN)\n",
    "body_CNN = conv_22(body_CNN)\n",
    "body_CNN = pool_4(body_CNN)\n",
    "body_CNN = Flatten()(body_CNN)\n",
    "\n",
    "conc = concatenate([head_CNN,body_CNN])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 335s 7ms/step - loss: 0.6402 - acc: 0.7650 - val_loss: 0.8381 - val_acc: 0.7034\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 411s 8ms/step - loss: 0.4362 - acc: 0.8304 - val_loss: 0.7601 - val_acc: 0.7036\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 372s 7ms/step - loss: 0.2951 - acc: 0.8849 - val_loss: 0.8014 - val_acc: 0.6828\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 254s 5ms/step - loss: 0.2163 - acc: 0.9154 - val_loss: 0.8790 - val_acc: 0.6946\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    563    |    10     |    488    |    842    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    123    |    14     |    144    |    416    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    485    |    22     |   2165    |   1792    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   1106    |    22     |   2312    |   14909   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6787.25 out of 11651.25\t(58.25340628687909%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 239s 5ms/step - loss: 0.1765 - acc: 0.9324 - val_loss: 0.9520 - val_acc: 0.6895\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 352s 7ms/step - loss: 0.1443 - acc: 0.9452 - val_loss: 1.0419 - val_acc: 0.6911\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 355s 7ms/step - loss: 0.1249 - acc: 0.9524 - val_loss: 1.0398 - val_acc: 0.7093\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 238s 5ms/step - loss: 0.1058 - acc: 0.9606 - val_loss: 1.1971 - val_acc: 0.7208\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    754    |    15     |    308    |    826    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    215    |    15     |    94     |    373    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    600    |    13     |   1758    |   2093    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   1268    |    41     |   1249    |   15791   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6786.0 out of 11651.25\t(58.24267782426778%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "14464/49972 [=======>......................] - ETA: 1:52 - loss: 0.1069 - acc: 0.9605"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=4, batch_size=128,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
