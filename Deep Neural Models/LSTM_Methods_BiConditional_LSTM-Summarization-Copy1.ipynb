{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from fncbaseline import feature_engineering\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = True\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = False, True\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "14847\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = False\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "30\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text[0]))\n",
    "print(len(train_body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'trainFull')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'testFull')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_labels(train_dataset)\n",
    "y_test = create_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t1\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_features = np.load('train_features_sentiment.npy')\n",
    "test_sentiment_features = np.load('test_features_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             4454400     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 200), (None, 320800      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          320800      embedding_1[0][0]                \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 401)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          40200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,136,604\n",
      "Trainable params: 682,204\n",
      "Non-trainable params: 4,454,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 580.00 629.00\" width=\"580pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 576,-625 576,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140560973789784 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140560973789784</title>\n",
       "<polygon fill=\"none\" points=\"92,-584.5 92,-620.5 277,-620.5 277,-584.5 92,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-598.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140560973790568 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140560973790568</title>\n",
       "<polygon fill=\"none\" points=\"184,-511.5 184,-547.5 387,-547.5 387,-511.5 184,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"285.5\" y=\"-525.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140560973789784&#45;&gt;140560973790568 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140560973789784-&gt;140560973790568</title>\n",
       "<path d=\"M209.4663,-584.4551C222.3342,-575.1545 238.2038,-563.6844 252.1102,-553.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"254.4164,-556.2849 260.4709,-547.5904 250.3159,-550.6116 254.4164,-556.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560973789896 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140560973789896</title>\n",
       "<polygon fill=\"none\" points=\"295,-584.5 295,-620.5 480,-620.5 480,-584.5 295,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-598.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140560973789896&#45;&gt;140560973790568 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140560973789896-&gt;140560973790568</title>\n",
       "<path d=\"M362.2865,-584.4551C349.2912,-575.1545 333.2645,-563.6844 319.2204,-553.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"320.9459,-550.5641 310.7769,-547.5904 316.8719,-556.2565 320.9459,-550.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560974273784 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140560974273784</title>\n",
       "<polygon fill=\"none\" points=\"209,-438.5 209,-474.5 572,-474.5 572,-438.5 209,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390.5\" y=\"-452.8\">bidirectional_1(body_lstm): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 140560973790568&#45;&gt;140560974273784 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140560973790568-&gt;140560974273784</title>\n",
       "<path d=\"M311.4551,-511.4551C324.9588,-502.0667 341.6422,-490.4678 356.1965,-480.3491\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"358.2669,-483.1725 364.4796,-474.5904 354.271,-477.425 358.2669,-483.1725\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560969797528 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140560969797528</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 363,-401.5 363,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-379.8\">bidirectional_2(head_lstm): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 140560973790568&#45;&gt;140560969797528 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140560973790568-&gt;140560969797528</title>\n",
       "<path d=\"M243.4406,-511.3078C227.7263,-502.5083 211.1326,-490.4521 200.5,-475 187.6961,-456.3924 183.1992,-430.9627 181.7609,-411.6341\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"185.2542,-411.4119 181.2431,-401.6057 178.2635,-411.7729 185.2542,-411.4119\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560974273784&#45;&gt;140560969797528 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140560974273784-&gt;140560969797528</title>\n",
       "<path d=\"M338.8371,-438.4551C309.8229,-428.3209 273.4343,-415.611 242.9619,-404.9675\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.8878,-401.5836 233.2929,-401.5904 241.5795,-408.1921 243.8878,-401.5836\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560973789616 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140560973789616</title>\n",
       "<polygon fill=\"none\" points=\"256,-292.5 256,-328.5 347,-328.5 347,-292.5 256,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-306.8\">dot_1: Dot</text>\n",
       "</g>\n",
       "<!-- 140560974273784&#45;&gt;140560973789616 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140560974273784-&gt;140560973789616</title>\n",
       "<path d=\"M393.1753,-438.4302C394.7671,-423.4274 395.6045,-401.6807 390.5,-383.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<path d=\"M390.5,-383.5C384.5578,-362.3358 367.29,-345.8815 349.5266,-333.9971\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"351.1448,-330.8805 340.8141,-328.5294 347.4238,-336.8097 351.1448,-330.8805\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560968845576 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140560968845576</title>\n",
       "<polygon fill=\"none\" points=\"191,-219.5 191,-255.5 412,-255.5 412,-219.5 191,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-233.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140560974273784&#45;&gt;140560968845576 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140560974273784-&gt;140560968845576</title>\n",
       "<path d=\"M390.5,-383.5C378.7304,-341.5807 379.108,-328.5844 355.5,-292 348.6728,-281.4202 339.5245,-271.2225 330.7002,-262.5673\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"333.0929,-260.0128 323.4221,-255.6853 328.2834,-265.099 333.0929,-260.0128\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560969797528&#45;&gt;140560973789616 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140560969797528-&gt;140560973789616</title>\n",
       "<path d=\"M211.1629,-365.4551C226.74,-355.979 246.0193,-344.2508 262.7617,-334.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"265.0381,-336.7778 271.7624,-328.5904 261.4,-330.7974 265.0381,-336.7778\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560969797528&#45;&gt;140560968845576 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140560969797528-&gt;140560968845576</title>\n",
       "<path d=\"M193.0927,-365.284C205.469,-346.3147 226.0722,-316.0659 246.5,-292 254.9952,-281.9919 265.0126,-271.6885 274.1547,-262.7716\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"276.7876,-265.0957 281.5755,-255.6444 271.9387,-260.047 276.7876,-265.0957\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560973789616&#45;&gt;140560968845576 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140560973789616-&gt;140560968845576</title>\n",
       "<path d=\"M301.5,-292.4551C301.5,-284.3828 301.5,-274.6764 301.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"305.0001,-265.5903 301.5,-255.5904 298.0001,-265.5904 305.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560968845464 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140560968845464</title>\n",
       "<polygon fill=\"none\" points=\"237.5,-146.5 237.5,-182.5 365.5,-182.5 365.5,-146.5 237.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140560968845576&#45;&gt;140560968845464 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140560968845576-&gt;140560968845464</title>\n",
       "<path d=\"M301.5,-219.4551C301.5,-211.3828 301.5,-201.6764 301.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"305.0001,-192.5903 301.5,-182.5904 298.0001,-192.5904 305.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560965019352 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140560965019352</title>\n",
       "<polygon fill=\"none\" points=\"223,-73.5 223,-109.5 380,-109.5 380,-73.5 223,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140560968845464&#45;&gt;140560965019352 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140560968845464-&gt;140560965019352</title>\n",
       "<path d=\"M301.5,-146.4551C301.5,-138.3828 301.5,-128.6764 301.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"305.0001,-119.5903 301.5,-109.5904 298.0001,-119.5904 305.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140560964251208 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140560964251208</title>\n",
       "<polygon fill=\"none\" points=\"237.5,-.5 237.5,-36.5 365.5,-36.5 365.5,-.5 237.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140560965019352&#45;&gt;140560964251208 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140560965019352-&gt;140560964251208</title>\n",
       "<path d=\"M301.5,-73.4551C301.5,-65.3828 301.5,-55.6764 301.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"305.0001,-46.5903 301.5,-36.5904 298.0001,-46.5904 305.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "\n",
    "body_lstm,state_h,state_c,x,y = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='body_lstm',return_state=True))(body_embed)\n",
    "encoded_states = [state_h,state_c,x,y]\n",
    "head_lstm = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='head_lstm'))(head_embed,initial_state=encoded_states)\n",
    "dot_layer = dot([head_lstm,body_lstm],axes = 1, normalize=True)\n",
    "conc = concatenate([head_lstm,body_lstm,dot_layer])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 203s 4ms/step - loss: 0.6871 - acc: 0.7530\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 198s 4ms/step - loss: 0.4484 - acc: 0.8256\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    256    |     1     |    309    |   1337    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    58     |     0     |    84     |    555    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    164    |     2     |   1467    |   2831    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    305    |     7     |    885    |   17152   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6165.5 out of 11651.25\t(52.91706898401459%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 199s 4ms/step - loss: 0.3124 - acc: 0.8809\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 204s 4ms/step - loss: 0.2328 - acc: 0.9137\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    512    |     0     |    486    |    905    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    136    |     1     |    146    |    414    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    542    |     3     |   2284    |   1635    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    638    |     0     |   1310    |   16401   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7225.5 out of 11651.25\t(62.01480527840361%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 205s 4ms/step - loss: 0.1764 - acc: 0.9336\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 205s 4ms/step - loss: 0.1401 - acc: 0.9480\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    549    |    17     |    639    |    698    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    140    |     3     |    208    |    346    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    572    |    11     |   2413    |   1468    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    685    |     7     |   1366    |   16291   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7434.5 out of 11651.25\t(63.80860422701427%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 205s 4ms/step - loss: 0.1172 - acc: 0.9561\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 204s 4ms/step - loss: 0.0973 - acc: 0.9637\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    507    |     8     |    519    |    869    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    107    |     2     |    155    |    433    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    578    |    10     |   2303    |   1573    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    711    |    14     |   1039    |   16585   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7302.5 out of 11651.25\t(62.675678575260164%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 203s 4ms/step - loss: 0.0821 - acc: 0.9684\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 203s 4ms/step - loss: 0.0731 - acc: 0.9721\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    581    |    10     |    408    |    904    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    128    |     1     |    126    |    442    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    697    |     7     |   2122    |   1638    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    757    |     8     |    751    |   16833   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7256.25 out of 11651.25\t(62.27872545864177%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],epochs=2, batch_size=128,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
