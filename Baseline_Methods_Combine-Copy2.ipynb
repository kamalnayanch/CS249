{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from fncbaseline import feature_engineering\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = False\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e671ecc85398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fiting a tokenizer on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = False\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "500\n",
      "49972\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_body[0]))\n",
    "print(len(train_dataset.stances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'trainFull')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'testFull')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_labels(train_dataset)\n",
    "y_test = create_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train.npy',y_train)\n",
    "np.save('y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t1\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_features = np.load('train_features_sentiment.npy')\n",
    "test_sentiment_features = np.load('test_features_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1875 0.25   0.25   0.25  ]\n",
      " [0.25   0.     0.1875 0.1875]\n",
      " [0.25   0.1875 0.     0.1875]\n",
      " [0.25   0.1875 0.1875 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from itertools import product\n",
    "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "w_array = np.array([[0.75,1,1,1],[1,0,0.75,0.75],[1,0.75,0,0.75],[1,0.75,0.75,0]]) \n",
    "print(w_array/4)\n",
    "#w_array = np.array([[0.75,1,1,1],[1,0,0.75,0.25],[1,0.75,0,0.75],[1,0.75,0.75,0]])\n",
    "ncce = functools.partial(w_categorical_crossentropy,weights=w_array/4)\n",
    "ncce.__name__ ='w_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             8513700     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 300)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10001)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10605)        0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          1060600     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            404         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,574,704\n",
      "Trainable params: 1,061,004\n",
      "Non-trainable params: 8,513,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "head_embed = Lambda(adder,output_shape=adder_output)(head_embed)\n",
    "body_embed = Lambda(adder,output_shape=adder_output)(body_embed)\n",
    "\n",
    "dot_layer = dot([head_embed,body_embed],axes = 1, normalize=True)\n",
    "\n",
    "comp_input = Input(shape=(train_features.shape[1],))\n",
    "\n",
    "tf_input = Input(shape = (train_tf_features.shape[1],))\n",
    "tf_dense = Dense(100,activation='relu')(tf_input)\n",
    "tf_dense = Dropout(0.4)(tf_dense)\n",
    "tf_dense = Dropout(0.4)(tf_dense)\n",
    "\n",
    "sent_input = Input(shape=(train_sentiment_features.shape[1],))\n",
    "\n",
    "conc = concatenate([head_embed,body_embed,dot_layer,tf_input,sent_input])\n",
    "\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.4)(dense)\n",
    "dense = Dropout(0.4)(dense)\n",
    "dense = Dropout(0.4)(dense)\n",
    "\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[head_input,body_input,comp_input,tf_input,sent_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 81s 2ms/step - loss: 1.1251 - acc: 0.7304 - val_loss: 1.1205 - val_acc: 0.7220\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 81s 2ms/step - loss: 1.0832 - acc: 0.7313 - val_loss: 1.1202 - val_acc: 0.7220\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 77s 2ms/step - loss: 1.0829 - acc: 0.7313 - val_loss: 1.1201 - val_acc: 0.7220\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 77s 2ms/step - loss: 1.0828 - acc: 0.7313 - val_loss: 1.1201 - val_acc: 0.7220\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |     0     |   1903    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |     0     |    697    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     0     |     0     |     0     |   4464    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |     0     |   18349   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4587.25 out of 11651.25\t(39.37131209097736%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 76s 2ms/step - loss: 1.0828 - acc: 0.7313 - val_loss: 1.1201 - val_acc: 0.7220\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 1.0827 - acc: 0.7313 - val_loss: 1.1201 - val_acc: 0.7220\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 72s 1ms/step - loss: 1.0044 - acc: 0.7473 - val_loss: 0.6485 - val_acc: 0.8183\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 75s 2ms/step - loss: 0.3857 - acc: 0.8650 - val_loss: 0.3857 - val_acc: 0.8602\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    769    |     0     |    880    |    254    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    157    |     0     |    326    |    214    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    940    |     0     |   2943    |    581    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     4     |     0     |    197    |   18148   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8824.75 out of 11651.25\t(75.74080034331081%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 78s 2ms/step - loss: 0.2739 - acc: 0.8845 - val_loss: 0.3600 - val_acc: 0.8664\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2533 - acc: 0.8866 - val_loss: 0.3468 - val_acc: 0.8715\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 73s 1ms/step - loss: 0.2424 - acc: 0.8870 - val_loss: 0.3449 - val_acc: 0.8703\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 78s 2ms/step - loss: 0.2378 - acc: 0.8862 - val_loss: 0.3386 - val_acc: 0.8718\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    20     |     0     |   1742    |    141    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    11     |     0     |    543    |    143    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    23     |     0     |   4072    |    369    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     2     |     0     |    284    |   18063   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9187.5 out of 11651.25\t(78.85420019311233%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 78s 2ms/step - loss: 0.2328 - acc: 0.8861 - val_loss: 0.3443 - val_acc: 0.8712\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 80s 2ms/step - loss: 0.2363 - acc: 0.8832 - val_loss: 0.3405 - val_acc: 0.8706\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 78s 2ms/step - loss: 0.2290 - acc: 0.8849 - val_loss: 0.3450 - val_acc: 0.8694\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 77s 2ms/step - loss: 0.2277 - acc: 0.8843 - val_loss: 0.3317 - val_acc: 0.8720\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     2     |     0     |   1756    |    145    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     5     |     0     |    546    |    146    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     5     |     0     |   4090    |    369    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |    280    |   18069   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9187.25 out of 11651.25\t(78.85205450059007%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 77s 2ms/step - loss: 0.2293 - acc: 0.8830 - val_loss: 0.3350 - val_acc: 0.8718\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2250 - acc: 0.8844 - val_loss: 0.3492 - val_acc: 0.8688\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2219 - acc: 0.8855 - val_loss: 0.3403 - val_acc: 0.8707\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 66s 1ms/step - loss: 0.2243 - acc: 0.8835 - val_loss: 0.3768 - val_acc: 0.8452\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    518    |     0     |   1260    |    125    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    70     |     0     |    494    |    133    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |   1164    |     0     |   2982    |    318    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    11     |     0     |    359    |   17979   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8741.75 out of 11651.25\t(75.02843042591996%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 68s 1ms/step - loss: 0.2212 - acc: 0.8843 - val_loss: 0.4460 - val_acc: 0.7998\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2175 - acc: 0.8868 - val_loss: 0.3439 - val_acc: 0.8693\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2213 - acc: 0.8832 - val_loss: 0.3346 - val_acc: 0.8720\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2166 - acc: 0.8857 - val_loss: 0.4794 - val_acc: 0.7867\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1679    |     0     |    117    |    107    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    525    |     0     |    64     |    108    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |   3685    |     0     |    500    |    279    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    105    |     0     |    431    |   17813   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7730.0 out of 11651.25\t(66.34481278832743%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2125 - acc: 0.8874 - val_loss: 0.3367 - val_acc: 0.8705\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 71s 1ms/step - loss: 0.2131 - acc: 0.8865 - val_loss: 0.3803 - val_acc: 0.8422\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2141 - acc: 0.8857 - val_loss: 0.3987 - val_acc: 0.8298\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 69s 1ms/step - loss: 0.2123 - acc: 0.8866 - val_loss: 0.3353 - val_acc: 0.8710\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    96     |     0     |   1685    |    122    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    17     |     0     |    559    |    121    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    83     |     0     |   4081    |    300    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     4     |     0     |    387    |   17958   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9252.5 out of 11651.25\t(79.41208024890034%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2100 - acc: 0.8866 - val_loss: 0.3587 - val_acc: 0.8568\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 71s 1ms/step - loss: 0.2091 - acc: 0.8869 - val_loss: 0.3557 - val_acc: 0.8662\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2106 - acc: 0.8860 - val_loss: 0.3401 - val_acc: 0.8692\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 70s 1ms/step - loss: 0.2057 - acc: 0.8879 - val_loss: 0.3610 - val_acc: 0.8656\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    112    |     0     |   1696    |    95     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    36     |     0     |    563    |    98     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    82     |     0     |   4136    |    246    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     7     |     0     |    592    |   17750   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9279.75 out of 11651.25\t(79.64596073382684%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 77s 2ms/step - loss: 0.2034 - acc: 0.8887 - val_loss: 0.4446 - val_acc: 0.8016\n",
      "Epoch 2/4\n",
      "11072/49972 [=====>........................] - ETA: 44s - loss: 0.2022 - acc: 0.8894"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1e0be0eb601b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_tf_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_sentiment_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tf_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_sentiment_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mevaluate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tf_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_sentiment_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1219\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.fit([train_text,train_body,train_features,train_tf_features,train_sentiment_features],[y_train], epochs=2, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body,test_features,test_tf_features,test_sentiment_features],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
