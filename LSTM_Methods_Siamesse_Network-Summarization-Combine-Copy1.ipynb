{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from fncbaseline import feature_engineering\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = True\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = False, True\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "14847\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = False\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "30\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text[0]))\n",
    "print(len(train_body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'trainFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'trainFull')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'testFull'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'testFull')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_labels(train_dataset)\n",
    "y_test = create_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t1\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_features = np.load('train_features_sentiment.npy')\n",
    "test_sentiment_features = np.load('test_features_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 10001)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         multiple             4454400     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          1000200     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 200)          320800      embedding_5[0][0]                \n",
      "                                                                 embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 300)          0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 300)          0           embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 100)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1)            0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_5[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_10 (Dot)                    (None, 1)            0           lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 100)          0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1149)         0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_5[1][0]            \n",
      "                                                                 dot_9[0][0]                      \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 dot_10[0][0]                     \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 100)          115000      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 100)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            404         dropout_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,890,804\n",
      "Trainable params: 1,436,404\n",
      "Non-trainable params: 4,454,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 1190.00 556.00\" width=\"1190pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 1186,-552 1186,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140688569372008 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140688569372008</title>\n",
       "<polygon fill=\"none\" points=\"264,-511.5 264,-547.5 449,-547.5 449,-511.5 264,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356.5\" y=\"-525.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140688569372176 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140688569372176</title>\n",
       "<polygon fill=\"none\" points=\"356,-438.5 356,-474.5 559,-474.5 559,-438.5 356,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457.5\" y=\"-452.8\">embedding_5: Embedding</text>\n",
       "</g>\n",
       "<!-- 140688569372008&#45;&gt;140688569372176 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140688569372008-&gt;140688569372176</title>\n",
       "<path d=\"M381.4663,-511.4551C394.3342,-502.1545 410.2038,-490.6844 424.1102,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"426.4164,-483.2849 432.4709,-474.5904 422.3159,-477.6116 426.4164,-483.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688569372120 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140688569372120</title>\n",
       "<polygon fill=\"none\" points=\"467,-511.5 467,-547.5 652,-547.5 652,-511.5 467,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559.5\" y=\"-525.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140688569372120&#45;&gt;140688569372176 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140688569372120-&gt;140688569372176</title>\n",
       "<path d=\"M534.2865,-511.4551C521.2912,-502.1545 505.2645,-490.6844 491.2204,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"492.9459,-477.5641 482.7769,-474.5904 488.8719,-483.2565 492.9459,-477.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688032206408 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140688032206408</title>\n",
       "<polygon fill=\"none\" points=\"827,-511.5 827,-547.5 996,-547.5 996,-511.5 827,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"911.5\" y=\"-525.8\">input_14: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140688032206576 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140688032206576</title>\n",
       "<polygon fill=\"none\" points=\"843,-438.5 843,-474.5 980,-474.5 980,-438.5 843,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"911.5\" y=\"-452.8\">dense_13: Dense</text>\n",
       "</g>\n",
       "<!-- 140688032206408&#45;&gt;140688032206576 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140688032206408-&gt;140688032206576</title>\n",
       "<path d=\"M911.5,-511.4551C911.5,-503.3828 911.5,-493.6764 911.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"915.0001,-484.5903 911.5,-474.5904 908.0001,-484.5904 915.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688569350520 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140688569350520</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 363,-401.5 363,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-379.8\">bidirectional_5(head_lstm): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 140688569372176&#45;&gt;140688569350520 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140688569372176-&gt;140688569350520</title>\n",
       "<path d=\"M389.2753,-438.4551C350.0476,-428.0796 300.6122,-415.0043 259.8041,-404.2109\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"260.459,-400.7638 249.8964,-401.5904 258.669,-407.5311 260.459,-400.7638\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688568916456 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140688568916456</title>\n",
       "<polygon fill=\"none\" points=\"381.5,-365.5 381.5,-401.5 533.5,-401.5 533.5,-365.5 381.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457.5\" y=\"-379.8\">lambda_9: Lambda</text>\n",
       "</g>\n",
       "<!-- 140688569372176&#45;&gt;140688568916456 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140688569372176-&gt;140688568916456</title>\n",
       "<path d=\"M457.5,-438.4551C457.5,-430.3828 457.5,-420.6764 457.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"461.0001,-411.5903 457.5,-401.5904 454.0001,-411.5904 461.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688033265200 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140688033265200</title>\n",
       "<polygon fill=\"none\" points=\"552,-365.5 552,-401.5 713,-401.5 713,-365.5 552,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"632.5\" y=\"-379.8\">lambda_10: Lambda</text>\n",
       "</g>\n",
       "<!-- 140688569372176&#45;&gt;140688033265200 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140688569372176-&gt;140688033265200</title>\n",
       "<path d=\"M500.7584,-438.4551C524.6319,-428.4964 554.4682,-416.0504 579.7069,-405.5223\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"581.251,-408.6705 589.1327,-401.5904 578.556,-402.2101 581.251,-408.6705\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031951784 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140688031951784</title>\n",
       "<polygon fill=\"none\" points=\"828.5,-365.5 828.5,-401.5 994.5,-401.5 994.5,-365.5 828.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"911.5\" y=\"-379.8\">dropout_13: Dropout</text>\n",
       "</g>\n",
       "<!-- 140688032206576&#45;&gt;140688031951784 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140688032206576-&gt;140688031951784</title>\n",
       "<path d=\"M911.5,-438.4551C911.5,-430.3828 911.5,-420.6764 911.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"915.0001,-411.5903 911.5,-401.5904 908.0001,-411.5904 915.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688569372344 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140688569372344</title>\n",
       "<polygon fill=\"none\" points=\"237,-292.5 237,-328.5 328,-328.5 328,-292.5 237,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-306.8\">dot_9: Dot</text>\n",
       "</g>\n",
       "<!-- 140688569350520&#45;&gt;140688569372344 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140688569350520-&gt;140688569372344</title>\n",
       "<path d=\"M206.4663,-365.4551C219.3342,-356.1545 235.2038,-344.6844 249.1102,-334.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"251.4164,-337.2849 257.4709,-328.5904 247.3159,-331.6116 251.4164,-337.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031653840 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140688031653840</title>\n",
       "<polygon fill=\"none\" points=\"503,-219.5 503,-255.5 724,-255.5 724,-219.5 503,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-233.8\">concatenate_5: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140688569350520&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140688569350520-&gt;140688031653840</title>\n",
       "<path d=\"M290.001,-365.4634C342.4189,-354.1229 405.1487,-336.5759 457.5,-310.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<path d=\"M457.5,-310.5C468.0499,-305.2451 466.6113,-298.4131 476.5,-292 497.1712,-278.5941 521.6647,-267.5648 544.1843,-259.02\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"545.5273,-262.2552 553.6978,-255.5104 543.1045,-255.6878 545.5273,-262.2552\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688568916288 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140688568916288</title>\n",
       "<polygon fill=\"none\" points=\"485.5,-292.5 485.5,-328.5 585.5,-328.5 585.5,-292.5 485.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"535.5\" y=\"-306.8\">dot_10: Dot</text>\n",
       "</g>\n",
       "<!-- 140688568916456&#45;&gt;140688568916288 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140688568916456-&gt;140688568916288</title>\n",
       "<path d=\"M476.7809,-365.4551C486.4373,-356.4177 498.2824,-345.3319 508.7984,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"511.261,-337.979 516.1706,-328.5904 506.4777,-332.8681 511.261,-337.979\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688568916456&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140688568916456-&gt;140688031653840</title>\n",
       "<path d=\"M450.179,-365.2442C444.065,-346.7869 439.0089,-319.7103 457.5,-310.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688033265200&#45;&gt;140688568916288 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140688033265200-&gt;140688568916288</title>\n",
       "<path d=\"M608.5225,-365.4551C596.1641,-356.1545 580.9231,-344.6844 567.5674,-334.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"569.6326,-331.807 559.5379,-328.5904 565.4234,-337.4001 569.6326,-331.807\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688033265200&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140688033265200-&gt;140688031653840</title>\n",
       "<path d=\"M630.1321,-365.3042C626.8894,-340.3868 621.0341,-295.3938 617.2073,-265.9877\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"620.6591,-265.3898 615.8978,-255.9251 613.7176,-266.2932 620.6591,-265.3898\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031653672 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140688031653672</title>\n",
       "<polygon fill=\"none\" points=\"828.5,-292.5 828.5,-328.5 994.5,-328.5 994.5,-292.5 828.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"911.5\" y=\"-306.8\">dropout_14: Dropout</text>\n",
       "</g>\n",
       "<!-- 140688031951784&#45;&gt;140688031653672 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140688031951784-&gt;140688031653672</title>\n",
       "<path d=\"M911.5,-365.4551C911.5,-357.3828 911.5,-347.6764 911.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"915.0001,-338.5903 911.5,-328.5904 908.0001,-338.5904 915.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688569372344&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140688569372344-&gt;140688031653840</title>\n",
       "<path d=\"M328.1766,-300.4263C377.9216,-289.4554 458.7957,-271.6191 521.7615,-257.7323\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"522.6944,-261.1108 531.7059,-255.5392 521.1868,-254.2751 522.6944,-261.1108\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688568916288&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140688568916288-&gt;140688031653840</title>\n",
       "<path d=\"M554.7809,-292.4551C564.4373,-283.4177 576.2824,-272.3319 586.7984,-262.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"589.261,-264.979 594.1706,-255.5904 584.4777,-259.8681 589.261,-264.979\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688033153152 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140688033153152</title>\n",
       "<polygon fill=\"none\" points=\"641,-292.5 641,-328.5 810,-328.5 810,-292.5 641,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"725.5\" y=\"-306.8\">input_13: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140688033153152&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140688033153152-&gt;140688031653840</title>\n",
       "<path d=\"M697.8146,-292.4551C683.276,-282.979 665.282,-271.2508 649.6558,-261.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"651.5438,-258.1186 641.2551,-255.5904 647.7215,-263.983 651.5438,-258.1186\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031653672&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>140688031653672-&gt;140688031653840</title>\n",
       "<path d=\"M837.8371,-292.4551C795.1338,-281.9942 741.2269,-268.7888 696.9592,-257.9447\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.6353,-254.5069 687.0897,-255.527 695.9698,-261.3059 697.6353,-254.5069\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031652384 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140688031652384</title>\n",
       "<polygon fill=\"none\" points=\"1013,-292.5 1013,-328.5 1182,-328.5 1182,-292.5 1013,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1097.5\" y=\"-306.8\">input_15: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140688031652384&#45;&gt;140688031653840 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>140688031652384-&gt;140688031653840</title>\n",
       "<path d=\"M1012.9895,-293.6379C1009.7881,-293.0721 1006.6164,-292.524 1003.5,-292 913.6519,-276.8927 811.6603,-262.745 734.5168,-252.6518\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"734.6483,-249.1394 724.2797,-251.3172 733.7433,-256.0807 734.6483,-249.1394\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031325824 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140688031325824</title>\n",
       "<polygon fill=\"none\" points=\"545,-146.5 545,-182.5 682,-182.5 682,-146.5 545,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-160.8\">dense_14: Dense</text>\n",
       "</g>\n",
       "<!-- 140688031653840&#45;&gt;140688031325824 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>140688031653840-&gt;140688031325824</title>\n",
       "<path d=\"M613.5,-219.4551C613.5,-211.3828 613.5,-201.6764 613.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"617.0001,-192.5903 613.5,-182.5904 610.0001,-192.5904 617.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031236448 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140688031236448</title>\n",
       "<polygon fill=\"none\" points=\"530.5,-73.5 530.5,-109.5 696.5,-109.5 696.5,-73.5 530.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-87.8\">dropout_15: Dropout</text>\n",
       "</g>\n",
       "<!-- 140688031325824&#45;&gt;140688031236448 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>140688031325824-&gt;140688031236448</title>\n",
       "<path d=\"M613.5,-146.4551C613.5,-138.3828 613.5,-128.6764 613.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"617.0001,-119.5903 613.5,-109.5904 610.0001,-119.5904 617.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140688031042752 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140688031042752</title>\n",
       "<polygon fill=\"none\" points=\"545,-.5 545,-36.5 682,-36.5 682,-.5 545,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"613.5\" y=\"-14.8\">dense_15: Dense</text>\n",
       "</g>\n",
       "<!-- 140688031236448&#45;&gt;140688031042752 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>140688031236448-&gt;140688031042752</title>\n",
       "<path d=\"M613.5,-73.4551C613.5,-65.3828 613.5,-55.6764 613.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"617.0001,-46.5903 613.5,-36.5904 610.0001,-46.5904 617.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='head_lstm'))\n",
    "head_lstm = shared_lstm(head_embed)\n",
    "body_lstm = shared_lstm(body_embed)\n",
    "\n",
    "dot_layer = dot([head_lstm,body_lstm],axes = 1, normalize=True)\n",
    "\n",
    "head_embed_w2v = Lambda(adder,output_shape=adder_output)(head_embed)\n",
    "body_embed_w2v = Lambda(adder,output_shape=adder_output)(body_embed)\n",
    "\n",
    "dot_layer_w2v = dot([head_embed_w2v,body_embed_w2v],axes = 1, normalize=True)\n",
    "\n",
    "comp_input = Input(shape=(train_features.shape[1],))\n",
    "\n",
    "tf_input = Input(shape = (train_tf_features.shape[1],))\n",
    "tf_dense = Dense(100,activation='relu')(tf_input)\n",
    "tf_dense = Dropout(0.4)(tf_dense)\n",
    "tf_dense = Dropout(0.4)(tf_dense)\n",
    "\n",
    "sent_input = Input(shape=(train_sentiment_features.shape[1],))\n",
    "\n",
    "conc = concatenate([head_lstm,body_lstm,dot_layer,head_embed_w2v,body_embed_w2v,dot_layer_w2v,comp_input,tf_dense,sent_input])\n",
    "\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input,comp_input,tf_input,sent_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 261s 5ms/step - loss: 0.2591 - acc: 0.9074\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.1310 - acc: 0.9521\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1102    |    16     |    612    |    173    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    314    |    22     |    192    |    169    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    791    |    17     |   3236    |    420    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    41     |     6     |    272    |   18030   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9353.0 out of 11651.25\t(80.27464864284948%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0952 - acc: 0.9647\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0758 - acc: 0.9720\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1300    |     4     |    458    |    141    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    403    |     7     |    152    |    135    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |   1158    |     3     |   2987    |    316    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    114    |     9     |    306    |   17920   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9318.5 out of 11651.25\t(79.97854307477738%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0622 - acc: 0.9772\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0532 - acc: 0.9813\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1084    |    18     |    661    |    140    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    315    |    47     |    202    |    133    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    826    |    56     |   3251    |    331    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    72     |    17     |    306    |   17954   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9390.0 out of 11651.25\t(80.59221113614419%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 259s 5ms/step - loss: 0.0462 - acc: 0.9827\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0394 - acc: 0.9852\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    936    |    29     |    773    |    165    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    263    |    54     |    221    |    159    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    665    |    46     |   3391    |    362    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    42     |    12     |    299    |   17996   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9379.25 out of 11651.25\t(80.49994635768694%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0355 - acc: 0.9867\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0304 - acc: 0.9886\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    972    |    26     |    806    |    99     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    303    |    47     |    253    |    94     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    743    |    31     |   3448    |    242    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    153    |    32     |    415    |   17749   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9444.75 out of 11651.25\t(81.06211779851947%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body,train_features,train_tf_features,train_sentiment_features],[y_train],epochs=2, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body,test_features,test_tf_features,test_sentiment_features],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 255s 5ms/step - loss: 0.0292 - acc: 0.9892\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 256s 5ms/step - loss: 0.0244 - acc: 0.9910\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1142    |    24     |    628    |    109    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    356    |    64     |    170    |    107    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |   1007    |    60     |   3155    |    242    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    129    |    70     |    464    |   17686   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9343.75 out of 11651.25\t(80.1952580195258%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 256s 5ms/step - loss: 0.0229 - acc: 0.9912\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 257s 5ms/step - loss: 0.0217 - acc: 0.9922\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |   1026    |    15     |    733    |    129    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    322    |    50     |    199    |    126    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    813    |    52     |   3284    |    315    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    82     |    19     |    295    |   17953   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9381.75 out of 11651.25\t(80.52140328290956%)\n",
      "Epoch 1/2\n",
      "49972/49972 [==============================] - 256s 5ms/step - loss: 0.0186 - acc: 0.9933\n",
      "Epoch 2/2\n",
      "49972/49972 [==============================] - 256s 5ms/step - loss: 0.0157 - acc: 0.9941\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    947    |    10     |    840    |    106    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    298    |    28     |    271    |    100    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    746    |    17     |   3465    |    236    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    147    |    23     |    495    |   17684   |\n",
      "-------------------------------------------------------------\n",
      "Score: 9406.5 out of 11651.25\t(80.73382684261345%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model.fit([train_text,train_body,train_features,train_tf_features,train_sentiment_features],[y_train],epochs=2, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body,test_features,test_tf_features,test_sentiment_features],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
