{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from fncbaseline.utils import dataset, generate_test_splits, score\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = dataset.DataSet()\n",
    "test_dataset = dataset.DataSet('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = False\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the data\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "51655\n",
      "77068\n",
      "77972\n"
     ]
    }
   ],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n",
      "28378\n"
     ]
    }
   ],
   "source": [
    "# Fiting a tokenizer on it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Glove and Made Dict\n",
      "25497\n",
      "2881\n"
     ]
    }
   ],
   "source": [
    "temp_name = 'embedding_matrix_glove'\n",
    "create_w2vec = True\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    w2v_DIR = \"./fnc-1/glove.840B.300d.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) !=301:\n",
    "            continue\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Glove and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2195875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "25413\n",
      "49972\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(train_text))\n",
    "print(len(test_text))\n",
    "print(len(train_dataset.stances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hand_features(feat_fn,data,ids,isTest,name):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(stance['Headline'])\n",
    "            body.append(data.articles[int(stance['Body ID'])])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    \n",
    "    \n",
    "    features = feature_engineering.gen_or_load_feats(feat_fn, head ,body, './fnc-1/'+str(feat_fn.__name__)+name+'.npy')\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simplified_data(data):\n",
    "    head = list()\n",
    "    body = list()\n",
    "    for stance in data.stances:\n",
    "        head.append(stance['Headline'])\n",
    "        body.append(data.articles[int(stance['Body ID'])])\n",
    "    return (head,body)\n",
    "def get_unique_head(dataset):\n",
    "    \n",
    "    head = list()\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Headline'] not in head:\n",
    "            head.append(stance['Headline'])\n",
    "    return head\n",
    "\n",
    "def get_unique_body(dataset):\n",
    "    \n",
    "    body = list()\n",
    "    for i in dataset.articles.keys():\n",
    "        body.append(dataset.articles[i])\n",
    "\n",
    "    return body\n",
    "\n",
    "def get_tf_features(train_dataset,test_dataset):\n",
    "    \n",
    "    train_head,train_body = get_simplified_data(train_dataset)\n",
    "    test_head,test_body = get_simplified_data(test_dataset)\n",
    "    \n",
    "    train_unique_head = get_unique_head(train_dataset)\n",
    "    train_unique_body = get_unique_body(train_dataset)\n",
    "    test_unique_head = get_unique_head(test_dataset)\n",
    "    test_unique_body = get_unique_body(test_dataset)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(train_unique_head + train_unique_body)\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english').fit(train_unique_head + train_unique_body+test_unique_body+test_unique_head)\n",
    "    \n",
    "    train_head_features_tf = list()\n",
    "    train_body_features_tf = list()\n",
    "    train_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    \n",
    "    \n",
    "    for stance in train_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = train_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        train_head_features_tf.append(head_dict[heading][0])\n",
    "        train_body_features_tf.append(body_dict[body][0])\n",
    "        train_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    test_head_features_tf = list()\n",
    "    test_body_features_tf = list()\n",
    "    test_cosine = list()\n",
    "    head_dict = dict()\n",
    "    body_dict = dict()\n",
    "    for stance in test_dataset.stances:\n",
    "        heading = stance['Headline']\n",
    "        body = test_dataset.articles[int(stance['Body ID'])]\n",
    "        if heading not in head_dict:\n",
    "            head_dict[heading] = (tfreq_vectorizer.transform(bow_vectorizer.transform([heading])).toarray(),tfidf_vectorizer.transform([heading]).toarray())\n",
    "        if body not in body_dict:\n",
    "            body_dict[body] = (tfreq_vectorizer.transform(bow_vectorizer.transform([body])).toarray(),tfidf_vectorizer.transform([body]).toarray())\n",
    "        \n",
    "        test_head_features_tf.append(head_dict[heading][0])\n",
    "        test_body_features_tf.append(body_dict[body][0])\n",
    "        test_cosine.append(cosine_similarity(head_dict[heading][1],body_dict[body][1]))\n",
    "    \n",
    "    \n",
    "    return (train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Features\n",
    "train_features = np.hstack([create_hand_features(feature_engineering.hand_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,train_dataset,None,True,'train_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,train_dataset,None,True,'train_full')])\n",
    "\n",
    "test_features = np.hstack([create_hand_features(feature_engineering.hand_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.word_overlap_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.refuting_features,test_dataset,None,True,'test_full'),\n",
    "                            create_hand_features(feature_engineering.polarity_features,test_dataset,None,True,'test_full')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_head_features_tf,train_body_features_tf,train_cosine,test_head_features_tf,test_body_features_tf,test_cosine) = get_tf_features(train_dataset,test_dataset)\n",
    "\n",
    "def reshaping(temp):\n",
    "    t1 = np.array(temp)\n",
    "    t1 = np.reshape(t1,[t1.shape[0],t1.shape[2]])\n",
    "    return t\n",
    "\n",
    "train_head_features_tf = reshaping(train_head_features_tf)\n",
    "train_body_features_tf = reshaping(train_body_features_tf)\n",
    "train_cosine = reshaping(train_cosine)\n",
    "test_head_features_tf = reshaping(test_head_features_tf)\n",
    "test_body_features_tf = reshaping(test_body_features_tf)\n",
    "test_cosine = reshaping(test_cosine)\n",
    "train_tf_features = np.hstack([train_head_features_tf, train_cosine, train_body_features_tf])\n",
    "test_tf_features = np.hstack([test_head_features_tf, test_cosine, test_body_features_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {0:'unrelated', 1: 'agree', 2: 'disagree', 3: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    score.report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "head_input (InputLayer)         (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             8513700     head_input[0][0]                 \n",
      "                                                                 body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 300)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 601)          0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,574,304\n",
      "Trainable params: 60,604\n",
      "Non-trainable params: 8,513,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 396.00 556.00\" width=\"396pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 392,-552 392,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140598859648416 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140598859648416</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 185,-547.5 185,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-525.8\">head_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140598860311128 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140598860311128</title>\n",
       "<polygon fill=\"none\" points=\"92,-438.5 92,-474.5 295,-474.5 295,-438.5 92,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-452.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140598859648416&#45;&gt;140598860311128 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140598859648416-&gt;140598860311128</title>\n",
       "<path d=\"M117.4663,-511.4551C130.3342,-502.1545 146.2038,-490.6844 160.1102,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.4164,-483.2849 168.4709,-474.5904 158.3159,-477.6116 162.4164,-483.2849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598859647408 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140598859647408</title>\n",
       "<polygon fill=\"none\" points=\"203,-511.5 203,-547.5 388,-547.5 388,-511.5 203,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-525.8\">body_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140598859647408&#45;&gt;140598860311128 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140598859647408-&gt;140598860311128</title>\n",
       "<path d=\"M270.2865,-511.4551C257.2912,-502.1545 241.2645,-490.6844 227.2204,-480.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.9459,-477.5641 218.7769,-474.5904 224.8719,-483.2565 228.9459,-477.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140601744566760 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140601744566760</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-365.5 32.5,-401.5 184.5,-401.5 184.5,-365.5 32.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108.5\" y=\"-379.8\">lambda_1: Lambda</text>\n",
       "</g>\n",
       "<!-- 140598860311128&#45;&gt;140601744566760 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140598860311128-&gt;140601744566760</title>\n",
       "<path d=\"M172.4888,-438.4551C161.8636,-429.3299 148.8068,-418.1165 137.2645,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"139.4308,-405.4505 129.5641,-401.5904 134.8701,-410.7609 139.4308,-405.4505\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140601653266528 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140601653266528</title>\n",
       "<polygon fill=\"none\" points=\"202.5,-365.5 202.5,-401.5 354.5,-401.5 354.5,-365.5 202.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-379.8\">lambda_2: Lambda</text>\n",
       "</g>\n",
       "<!-- 140598860311128&#45;&gt;140601653266528 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140598860311128-&gt;140601653266528</title>\n",
       "<path d=\"M214.5112,-438.4551C225.1364,-429.3299 238.1932,-418.1165 249.7355,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"252.1299,-410.7609 257.4359,-401.5904 247.5692,-405.4505 252.1299,-410.7609\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598860310176 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140598860310176</title>\n",
       "<polygon fill=\"none\" points=\"153,-292.5 153,-328.5 244,-328.5 244,-292.5 153,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-306.8\">dot_1: Dot</text>\n",
       "</g>\n",
       "<!-- 140601744566760&#45;&gt;140598860310176 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140601744566760-&gt;140598860310176</title>\n",
       "<path d=\"M130.7472,-365.4551C142.1055,-356.2422 156.0883,-344.9006 168.3956,-334.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"170.6352,-337.6081 176.1968,-328.5904 166.2256,-332.1716 170.6352,-337.6081\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598860309056 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140598860309056</title>\n",
       "<polygon fill=\"none\" points=\"88,-219.5 88,-255.5 309,-255.5 309,-219.5 88,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-233.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140601744566760&#45;&gt;140598860309056 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140601744566760-&gt;140598860309056</title>\n",
       "<path d=\"M113.0076,-365.2421C118.3117,-345.9693 128.3617,-315.2248 143.5,-292 150.439,-281.3544 159.7529,-271.1395 168.7416,-262.4888\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.2023,-264.9803 176.1563,-255.615 166.4433,-259.8468 171.2023,-264.9803\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140601653266528&#45;&gt;140598860310176 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140601653266528-&gt;140598860310176</title>\n",
       "<path d=\"M258.7247,-365.4551C248.8207,-356.4177 236.6719,-345.3319 225.8862,-335.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.0711,-332.7455 218.3251,-328.5904 223.3527,-337.9163 228.0711,-332.7455\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140601653266528&#45;&gt;140598860309056 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140601653266528-&gt;140598860309056</title>\n",
       "<path d=\"M276.076,-365.2487C272.9079,-345.9813 266.0807,-315.2413 252.5,-292 246.2629,-281.3263 237.4505,-271.2285 228.7499,-262.692\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"230.8903,-259.9011 221.2003,-255.6169 226.1037,-265.0088 230.8903,-259.9011\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598860310176&#45;&gt;140598860309056 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140598860310176-&gt;140598860309056</title>\n",
       "<path d=\"M198.5,-292.4551C198.5,-284.3828 198.5,-274.6764 198.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-265.5903 198.5,-255.5904 195.0001,-265.5904 202.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598860310064 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140598860310064</title>\n",
       "<polygon fill=\"none\" points=\"134.5,-146.5 134.5,-182.5 262.5,-182.5 262.5,-146.5 134.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140598860309056&#45;&gt;140598860310064 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140598860309056-&gt;140598860310064</title>\n",
       "<path d=\"M198.5,-219.4551C198.5,-211.3828 198.5,-201.6764 198.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-192.5903 198.5,-182.5904 195.0001,-192.5904 202.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598861153952 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140598861153952</title>\n",
       "<polygon fill=\"none\" points=\"120,-73.5 120,-109.5 277,-109.5 277,-73.5 120,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140598860310064&#45;&gt;140598861153952 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140598860310064-&gt;140598861153952</title>\n",
       "<path d=\"M198.5,-146.4551C198.5,-138.3828 198.5,-128.6764 198.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-119.5903 198.5,-109.5904 195.0001,-119.5904 202.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140598861112768 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140598861112768</title>\n",
       "<polygon fill=\"none\" points=\"134.5,-.5 134.5,-36.5 262.5,-36.5 262.5,-.5 134.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140598861153952&#45;&gt;140598861112768 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140598861153952-&gt;140598861112768</title>\n",
       "<path d=\"M198.5,-73.4551C198.5,-65.3828 198.5,-55.6764 198.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"202.0001,-46.5903 198.5,-36.5904 195.0001,-46.5904 202.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "head_embed = Lambda(adder,output_shape=adder_output)(head_embed)\n",
    "body_embed = Lambda(adder,output_shape=adder_output)(body_embed)\n",
    "dot = dot([head_embed,body_embed],axes = 1, normalize=True)\n",
    "conc = concatenate([head_embed,body_embed,dot])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.33)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[head_input, body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 14s 281us/step - loss: 0.6960 - acc: 0.7503 - val_loss: 0.6466 - val_acc: 0.7621\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.4840 - acc: 0.8239 - val_loss: 0.5456 - val_acc: 0.7972\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.3864 - acc: 0.8635 - val_loss: 0.4945 - val_acc: 0.8243\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.3362 - acc: 0.8804 - val_loss: 0.5035 - val_acc: 0.8183\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.3031 - acc: 0.8915 - val_loss: 0.4927 - val_acc: 0.8273\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 14s 282us/step - loss: 0.2790 - acc: 0.8999 - val_loss: 0.4891 - val_acc: 0.8301\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.2615 - acc: 0.9060 - val_loss: 0.5007 - val_acc: 0.8293\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.2432 - acc: 0.9122 - val_loss: 0.5108 - val_acc: 0.8273\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 14s 273us/step - loss: 0.2318 - acc: 0.9151 - val_loss: 0.5055 - val_acc: 0.8288\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.2195 - acc: 0.9191 - val_loss: 0.5237 - val_acc: 0.8280\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    755    |    17     |    498    |    633    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    137    |    15     |    173    |    372    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    605    |     7     |   2605    |   1247    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    146    |     0     |    536    |   17667   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8151.0 out of 11651.25\t(69.9581589958159%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.2090 - acc: 0.9235 - val_loss: 0.5280 - val_acc: 0.8309\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.2026 - acc: 0.9256 - val_loss: 0.5132 - val_acc: 0.8293\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.1936 - acc: 0.9280 - val_loss: 0.5351 - val_acc: 0.8303\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1859 - acc: 0.9318 - val_loss: 0.5492 - val_acc: 0.8294\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 281us/step - loss: 0.1787 - acc: 0.9332 - val_loss: 0.5372 - val_acc: 0.8311\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1729 - acc: 0.9363 - val_loss: 0.5564 - val_acc: 0.8287\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 14s 276us/step - loss: 0.1681 - acc: 0.9375 - val_loss: 0.5840 - val_acc: 0.8262\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1633 - acc: 0.9399 - val_loss: 0.5858 - val_acc: 0.8288\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1566 - acc: 0.9422 - val_loss: 0.5790 - val_acc: 0.8288\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1534 - acc: 0.9425 - val_loss: 0.5820 - val_acc: 0.8274\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    852    |    36     |    544    |    471    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    203    |    20     |    206    |    268    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    648    |    31     |   2831    |    954    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    190    |     4     |    831    |   17324   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8451.0 out of 11651.25\t(72.53299002252977%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.1466 - acc: 0.9466 - val_loss: 0.5914 - val_acc: 0.8260\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.1439 - acc: 0.9469 - val_loss: 0.5962 - val_acc: 0.8265\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1391 - acc: 0.9493 - val_loss: 0.5934 - val_acc: 0.8272\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1359 - acc: 0.9489 - val_loss: 0.6062 - val_acc: 0.8283\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.1360 - acc: 0.9498 - val_loss: 0.6557 - val_acc: 0.8279\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.1292 - acc: 0.9519 - val_loss: 0.6242 - val_acc: 0.8268\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 14s 288us/step - loss: 0.1274 - acc: 0.9524 - val_loss: 0.6335 - val_acc: 0.8277\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.1242 - acc: 0.9533 - val_loss: 0.6387 - val_acc: 0.8228\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 14s 271us/step - loss: 0.1221 - acc: 0.9545 - val_loss: 0.6644 - val_acc: 0.8247\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1185 - acc: 0.9569 - val_loss: 0.6698 - val_acc: 0.8260\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    877    |    20     |    475    |    531    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    178    |    17     |    174    |    328    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    658    |    40     |   2589    |   1177    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    191    |     4     |    645    |   17509   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8246.5 out of 11651.25\t(70.77781353931981%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n",
      "49972/49972 [==============================] - 14s 273us/step - loss: 0.1166 - acc: 0.9574 - val_loss: 0.6638 - val_acc: 0.8251\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1126 - acc: 0.9589 - val_loss: 0.6699 - val_acc: 0.8252\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1112 - acc: 0.9579 - val_loss: 0.6798 - val_acc: 0.8256\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1099 - acc: 0.9594 - val_loss: 0.6842 - val_acc: 0.8265\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1057 - acc: 0.9609 - val_loss: 0.6753 - val_acc: 0.8259\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 14s 280us/step - loss: 0.1047 - acc: 0.9618 - val_loss: 0.6796 - val_acc: 0.8266\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.1024 - acc: 0.9632 - val_loss: 0.6996 - val_acc: 0.8230\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 14s 275us/step - loss: 0.1012 - acc: 0.9629 - val_loss: 0.7272 - val_acc: 0.8225\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 14s 274us/step - loss: 0.0989 - acc: 0.9632 - val_loss: 0.7013 - val_acc: 0.8255\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.0977 - acc: 0.9649 - val_loss: 0.7386 - val_acc: 0.8261\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    857    |    10     |    490    |    546    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    193    |    13     |    166    |    325    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    611    |    22     |   2700    |   1131    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    152    |     3     |    770    |   17424   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8299.0 out of 11651.25\t(71.22840896899474%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972/49972 [==============================] - 14s 281us/step - loss: 0.0933 - acc: 0.9654 - val_loss: 0.7259 - val_acc: 0.8223\n",
      "Epoch 2/10\n",
      "49972/49972 [==============================] - 14s 283us/step - loss: 0.0945 - acc: 0.9659 - val_loss: 0.7240 - val_acc: 0.8228\n",
      "Epoch 3/10\n",
      "49972/49972 [==============================] - 14s 278us/step - loss: 0.0921 - acc: 0.9670 - val_loss: 0.7458 - val_acc: 0.8250\n",
      "Epoch 4/10\n",
      "49972/49972 [==============================] - 14s 279us/step - loss: 0.0909 - acc: 0.9668 - val_loss: 0.7926 - val_acc: 0.8275\n",
      "Epoch 5/10\n",
      "49972/49972 [==============================] - 14s 286us/step - loss: 0.0925 - acc: 0.9663 - val_loss: 0.7786 - val_acc: 0.8218\n",
      "Epoch 6/10\n",
      "49972/49972 [==============================] - 14s 290us/step - loss: 0.0883 - acc: 0.9675 - val_loss: 0.7596 - val_acc: 0.8259\n",
      "Epoch 7/10\n",
      "49972/49972 [==============================] - 14s 283us/step - loss: 0.0859 - acc: 0.9690 - val_loss: 0.7656 - val_acc: 0.8245\n",
      "Epoch 8/10\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.0865 - acc: 0.9688 - val_loss: 0.7785 - val_acc: 0.8201\n",
      "Epoch 9/10\n",
      "49972/49972 [==============================] - 14s 277us/step - loss: 0.0828 - acc: 0.9703 - val_loss: 0.7988 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "49972/49972 [==============================] - 14s 281us/step - loss: 0.0821 - acc: 0.9698 - val_loss: 0.7788 - val_acc: 0.8273\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    865    |    14     |    548    |    476    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    223    |     8     |    185    |    281    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    559    |    35     |   2860    |   1010    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    169    |     7     |    881    |   17292   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8447.0 out of 11651.25\t(72.49865894217359%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=10, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 14s 286us/step - loss: 0.0824 - acc: 0.9702 - val_loss: 0.8183 - val_acc: 0.8249\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 291us/step - loss: 0.0797 - acc: 0.9708 - val_loss: 0.8515 - val_acc: 0.8243\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 302us/step - loss: 0.0794 - acc: 0.9715 - val_loss: 0.8030 - val_acc: 0.8241\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0766 - acc: 0.9725 - val_loss: 0.8679 - val_acc: 0.8241\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0748 - acc: 0.9727 - val_loss: 0.8414 - val_acc: 0.8278\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    869    |    30     |    483    |    521    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    216    |    14     |    160    |    307    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    615    |    40     |   2660    |   1149    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    170    |     9     |    676    |   17494   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8302.5 out of 11651.25\t(71.2584486643064%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 296us/step - loss: 0.0765 - acc: 0.9722 - val_loss: 0.8333 - val_acc: 0.8247\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0761 - acc: 0.9723 - val_loss: 0.8282 - val_acc: 0.8245\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.0738 - acc: 0.9725 - val_loss: 0.8624 - val_acc: 0.8240\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0728 - acc: 0.9735 - val_loss: 0.8839 - val_acc: 0.8231\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 16s 316us/step - loss: 0.0719 - acc: 0.9744 - val_loss: 0.8416 - val_acc: 0.8221\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    893    |    34     |    486    |    490    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    237    |    21     |    147    |    292    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    672    |    46     |   2737    |   1009    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    184    |    19     |    906    |   17240   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8366.5 out of 11651.25\t(71.80774595000537%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 303us/step - loss: 0.0694 - acc: 0.9757 - val_loss: 0.8744 - val_acc: 0.8245\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0696 - acc: 0.9751 - val_loss: 0.8848 - val_acc: 0.8264\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 305us/step - loss: 0.0687 - acc: 0.9758 - val_loss: 0.9245 - val_acc: 0.8231\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 307us/step - loss: 0.0688 - acc: 0.9752 - val_loss: 0.9044 - val_acc: 0.8225\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 304us/step - loss: 0.0656 - acc: 0.9759 - val_loss: 0.8853 - val_acc: 0.8230\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    810    |    20     |    518    |    555    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    199    |    12     |    164    |    322    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    568    |    42     |   2762    |   1092    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    149    |    14     |    854    |   17332   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8294.75 out of 11651.25\t(71.1919321961163%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 300us/step - loss: 0.0665 - acc: 0.9758 - val_loss: 0.9242 - val_acc: 0.8203\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0664 - acc: 0.9756 - val_loss: 0.9261 - val_acc: 0.8215\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0639 - acc: 0.9764 - val_loss: 0.9106 - val_acc: 0.8218\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.0666 - acc: 0.9754 - val_loss: 0.9188 - val_acc: 0.8238\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0638 - acc: 0.9768 - val_loss: 0.9339 - val_acc: 0.8220\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    910    |    10     |    453    |    530    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    218    |     8     |    157    |    314    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    670    |    34     |   2648    |   1112    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    212    |    10     |    804    |   17323   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8282.25 out of 11651.25\t(71.08464757000321%)\n",
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/5\n",
      "49972/49972 [==============================] - 15s 299us/step - loss: 0.0642 - acc: 0.9765 - val_loss: 0.9688 - val_acc: 0.8199\n",
      "Epoch 2/5\n",
      "49972/49972 [==============================] - 15s 298us/step - loss: 0.0608 - acc: 0.9783 - val_loss: 0.9661 - val_acc: 0.8175\n",
      "Epoch 3/5\n",
      "49972/49972 [==============================] - 15s 295us/step - loss: 0.0616 - acc: 0.9778 - val_loss: 0.9158 - val_acc: 0.8206\n",
      "Epoch 4/5\n",
      "49972/49972 [==============================] - 15s 296us/step - loss: 0.0617 - acc: 0.9776 - val_loss: 0.9399 - val_acc: 0.8223\n",
      "Epoch 5/5\n",
      "49972/49972 [==============================] - 16s 324us/step - loss: 0.0603 - acc: 0.9787 - val_loss: 0.9342 - val_acc: 0.8233\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    877    |    20     |    470    |    536    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    221    |    10     |    152    |    314    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    656    |    30     |   2599    |   1179    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    191    |    16     |    705    |   17437   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8232.5 out of 11651.25\t(70.65765475807316%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=5, batch_size=64,verbose = True)\n",
    "    evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
